{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Mixture Models, and the EM algorithm\n",
    "\n",
    "AM207: Pavlos Protopapas, Harvard University\n",
    "\n",
    "April 17, 2014\n",
    "\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "import seaborn as sns\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\newcommand{\\isum}{\\sum_{i}}$$\n",
    "$$\\newcommand{\\zsum}{\\sum_{k=1}^{K}}$$\n",
    "$$\\newcommand{\\zsumi}{\\sum_{\\{z_i\\}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Mixture models\n",
    "\n",
    "It is common to assume that observations are correlated due to some common “cause”. Hierarchical bayesian models are an example where we assume that information flows between observations through a tied-together set of higher level hyper-parameters. \n",
    "\n",
    "We can also construct models with 'hidden' or 'augmented' variables, also known as latent variable models, which may or may not correlate with a cause. Since such models often have fewer parameters than observations, they are useful in modelling many problems.\n",
    "\n",
    "An example of a hidden model is the mixture model. A distribution $p(x| {\\theta_{k}})$ is a mixture of $K$ component distributions $p_1, p_2,... p_K$ if:\n",
    "\n",
    "$$p(x| \\{\\theta_{k}\\}) = \\zsum \\lambda_k p_{k}(x | \\theta_k)$$\n",
    "\n",
    "with the $\\lambda_k$ being mixing weights, $\\lambda_k > 0$, $\\zsum \\lambda_k = 1$.\n",
    "\n",
    "The $p_k$'s can be completely arbitrary, but we usually assume they are from the same family, like Gaussians with different centers and variances, or Poissons with different means.\n",
    "\n",
    "The way to generate a new observation from such a distribution thus would be the following:\n",
    "\n",
    "$$Z \\sim Mult(\\lambda_1,\\lambda_2,...,\\lambda_K)$$\n",
    "\n",
    "$$x | z  = p_k$$\n",
    "\n",
    "where $Z$ says which component X is drawn from. Thus $\\lambda_j$ is the probability that the hidden class variable Z is j, and we are setting a multinomial prior. \n",
    "\n",
    "Thus we can see the general structure above:\n",
    "\n",
    "$$p(x | \\theta) = \\sum_z p(z)p(x | z, \\theta)$$\n",
    "\n",
    "where $\\theta = \\{ \\theta_k \\}$ is the collection of distribution parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Gaussian Mixture Models\n",
    "\n",
    "The Gaussian mixture model or GMM is the most widely used mixture distribution. In this model, each base distribution  is a multivariate Gaussian with mean $\\mu_k$ and covariance matrix $\\Sigma_k$. Thus the model has the form\n",
    "\n",
    "$$p(x| \\{\\theta_{k}\\}) = \\zsum \\lambda_k N(x \\vert \\mu_k , \\Sigma_k ) $$\n",
    "\n",
    "Thus each mixture component is represented by a different set of eliptical contours, and we add these to create our overall density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAECCAYAAAAFL5eMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nklQgy12SCmS5S1KB/gd72oolMGpHLAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108d10588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#In 1-D\n",
    "# True parameter values\n",
    "mu_true = [2, 5]\n",
    "sigma_true = [0.6, 0.6]\n",
    "lambda_true = .4\n",
    "n = 1000\n",
    "\n",
    "# Simulate from each distribution according to mixing proportion psi\n",
    "z = np.random.binomial(1, lambda_true, n)\n",
    "x = np.array([np.random.normal(mu_true[i], sigma_true[i]) for i in z])\n",
    "\n",
    "plt.hist(x, bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Supervised learning: Gaussian Discriminant Analysis\n",
    "\n",
    "In supervised learning, we have some training data of $N$ samples, each with $m$ features, which we use to train a \"generative\" or \"discriminative\" classifier. \n",
    "\n",
    "For a feature vector x, we use Bayes rule to express the posterior of the class-conditional as:\n",
    "\n",
    "$$p(z = c|x, \\theta) = \\frac{p(z = c | \\theta)p(x | z = c, \\theta)}{ \\sum_{c′} p(z = c′ | \\theta) p(x | z = c′, \\theta)}$$\n",
    "\n",
    "This is a generative classifier, since it specifies how to generate the data using the class-conditional density $p(x|z = c, \\theta)$ and the class prior $p(z = c\\vert \\theta)$. \n",
    "\n",
    "An alternative approach is to directly fit the class posterior, $p(z = c|x, \\theta)$; this is known as a discriminative classifier. For example, Naive bayes is a generative classifier whose discriminative counterpart is the logistic regression.\n",
    "\n",
    "In the context of the mixture model, the supervised learning case is the one in which where hidden variables $z$ are known. \n",
    "\n",
    "Suppose we have input data \n",
    "$x$ are continuous-valued random variables, and $z$ labels.  We can use \n",
    "maximum likelihood approach that models the **full-data** likelihood $p(x,z | \\theta)$ using a multivariate normal distribution. The model is\n",
    "\n",
    "$$ z \\sim \\rm{Bernoulli}(\\lambda) $$\n",
    "$$ x|z=0 \\sim {\\cal N}(\\mu_0, \\Sigma_0) $$\n",
    "$$ x|z=1 \\sim {\\cal N}(\\mu_1, \\Sigma_1) $$\n",
    "\n",
    "The distributions in details are\n",
    "$$ p(z) = \\lambda^z (1-\\lambda)^{1-z}$$\n",
    "$$ p(x|z=0) = \\frac{1}{(2\\pi)^{n/2} | \\Sigma|^{1/2}} \\exp \\left( -\\frac{1}{2}(x-\\mu_0)^T \\,\\Sigma^{-1}(x-\\mu_0) \\right) $$\n",
    "$$ p(x|z=1) = \\frac{1}{(2\\pi)^{n/2} | \\Sigma|^{1/2}} \\exp \\left( -\\frac{1}{2}(x-\\mu_1)^T \\,\\Sigma^{-1}(x-\\mu_1) \\right) $$\n",
    "\n",
    "\n",
    "where the parameters of the model $\\lambda$, $\\Sigma$, $\\mu_0$ and $\\mu_1$ are to be determined. \n",
    "Note for simplicity of exposition we use one covariance matrix for both Gaussians. \n",
    "The log-likelihood of the data is given \n",
    "\n",
    "\\begin{eqnarray}\n",
    " l(x,z| \\lambda,\\mu_0, \\mu_1, \\Sigma) &=& \\log \\prod_{i=1}^{m} p(x_i,z_i| \\lambda, \\mu_0, \\mu_1, \\Sigma) \\nonumber \\\\ \n",
    "          &=& \\sum_{i=1}^{m} \\log \\left[p(x_i|z_i,  \\mu_0, \\mu_1, \\Sigma) \\,p(z_i| \\lambda) \\right]  \\nonumber \\\\ \n",
    "          &=& \\sum_{i=1}^{m} \\log p(x_i|z_i,  \\mu_0, \\mu_1, \\Sigma) + \\sum_{i=1}^{m}  \\log p(z_i| \\lambda)   \\nonumber   \\\\      \n",
    "\t&=&  -\\sum_{i=1}^{m} \\log ((2\\pi)^{n/2} | \\Sigma|^{1/2}) - \\frac{1}{2} \\sum_{i=1}^{m}  (x-\\mu_{z_i})^T \\,\\Sigma^{-1}(x-\\mu_{z_i})   \\nonumber \\\\ \n",
    "\t\t& & \\quad \\quad +\\sum_{i=1}^{m} \\left[ z_i \\, \\log \\lambda + (1-z_i) \\log(1-\\lambda) \\right]\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\n",
    "Taking derivatives with respect to  $\\lambda$, $\\Sigma$, $\\mu_0$ and $\\mu_1$ and setting them to zero we get \n",
    "\\begin{eqnarray}\n",
    "   \\lambda & = &\\frac{1}{m}  \\sum_{i=1}^{m}  \\delta_{z_i,1} \\nonumber  \\\\ \n",
    "   \\mu_0 &=& \\frac{ \\sum_{i=1}^{m}  \\delta_{z_i,0} \\, x_i  }{ \\sum_{i=1}^{m}   \\delta_{z_i,0}}\\nonumber  \\\\ \n",
    "    \\mu_1 &=& \\frac{ \\sum_{i=1}^{m}  \\delta_{z_i,1} \\, x_i  }{ \\sum_{i=1}^{m}   \\delta_{z_i,1}}\\nonumber  \\\\ \n",
    " \\Sigma &=&\\frac{1}{m}   \\sum_{i=1}^{m}  (x_i-\\mu_{z_i})   (x_i-\\mu_{z_i})^{T} \n",
    "\\end{eqnarray}\n",
    "\n",
    "This gives us the obvious result, namely $\\lambda$ is nothing more but the fraction of objects with label $z=1$ and the total \n",
    "number of objects, $\\mu$'s are the mean within the class and $\\Sigma$ is the the covariance matrix \n",
    "for each group. This analysis is called \"Gaussian Discriminant Analysis\" or GDA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the z's are the classes in the supervised learning\n",
    "#the 'feature' is the x position of the sample\n",
    "from sklearn.cross_validation import train_test_split\n",
    "ztrain, ztest, xtrain, xtest = train_test_split(z,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((750,), (750,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ztrain.shape, xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(xtrain);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambda_train=np.mean(ztrain)\n",
    "mu0_train = np.sum(xtrain[ztrain==0])/(np.sum(ztrain==0))\n",
    "mu1_train = np.sum(xtrain[ztrain==1])/(np.sum(ztrain==1))\n",
    "xmus=np.array([mu0_train if z==0 else mu1_train for z in ztrain])\n",
    "xdiffs = xtrain - xmus\n",
    "sigma_train=np.sqrt(np.dot(xdiffs, xdiffs)/xtrain.shape[0])\n",
    "print (lambda_train, mu0_train, mu1_train, sigma_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the log likelihood at a given `x` as a classifier: assign the class '0' or '1' depending upon which probability $p(x_j|\\lambda, \\mu_0,\\mu_1,\\Sigma)$ is larger. The first term of the likelihood does not matter since is independent of $z$, therefore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loglikdiff(x):\n",
    "    for0= - (x-mu0_train)*(x-mu0_train)/(2.0*sigma_train*sigma_train) \n",
    "    for0 = for0 + np.log(1.-lambda_train)\n",
    "    for1 = - (x-mu1_train)*(x-mu1_train)/(2.0*sigma_train*sigma_train) \n",
    "    for1 = for1 + np.log(lambda_train)\n",
    "    return 1*(for1 - for0 >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = np.array([loglikdiff(test_x) for test_x in xtest])\n",
    "print \"correct classification rate\", np.mean(ztest == pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning: Mixture of Gaussians \n",
    "\n",
    "In unsupervised learning, we do not know the class labels. We wish to generate these labels automatically from the data. An example of an unsupervised model is clustering.  In the context of mixture models we do not know what the components of the mixture model are, i.e. what the parameters of the components and their admixture ($\\lambda$s) are. Indeed, we might not even know how many components we have!!\n",
    "\n",
    "In this case, to carry out the clustering, we first fit the mixture model, and then compute $p(z_i = k | x_i, \\theta)$, which represents the posterior probability that point i belongs to cluster k. This is known as the responsibility of cluster k for point i, and can be computed as before using Bayes rule as follows:\n",
    "\n",
    "$$p(z_k = c|x_i, \\theta) = \\frac{p(z_k = c | \\theta)p(x_i | z_k = c, \\theta)}\n",
    "{ \\sum_{_c′} p(z_k = c′ | \\theta) p(x_i | z_k = c′, \\theta)}$$\n",
    "\n",
    "This is called soft clustering.\n",
    "\n",
    "The process is identical to the computations performed before in the supervised learning, except at training time: here we never observe $z_k$ for any samples, whereas before with the generative GDA classifier, we did observe $z_k$ on the training set.\n",
    "\n",
    "How many clusters? The best number will generalize best to future data, something we can use cross-validation or other techniques to find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Concretely formulating the problem\n",
    "\n",
    "So let us turn our attention to the case where we do not know the labels $z$. \n",
    "\n",
    "Suppose we are given a data set $\\{x_1,\\ldots, x_m\\}$ but not given the labels $z$. The model consists of \n",
    "$k$ Gaussians. In other words our model assumes that each $x_i$ was generated by randomly choosing \n",
    "$z_i$ from $\\{1, \\ldots, k\\}$, and then $x_i$ is drawn from one of the $k$ Gaussians depending on \n",
    "$z_i$. \n",
    "\n",
    "We wish to compute either the maximum likelihood estimate or the MAP estimate for this model, $p(\\{x_{i}\\} \\vert \\theta)$ or $p(\\theta | \\{x_{i}\\})$.\n",
    "The goal is to model the joint distribution $p(\\{x_i\\}, \\{z_i\\})=p(\\{x_i\\}|\\{z_i\\}) \\, p(\\{z_i\\})$ where $z_i \\sim \\rm{Multinomial}(\\lambda)$, and $\\lambda = \\{\\lambda_j\\}$.\n",
    "\n",
    "As in our definition of mixture models $\\lambda_j\\ge0$  and \n",
    "\n",
    "$$ \\sum_j^k \\lambda_i = 1 $$\n",
    "\n",
    "The parameters $\\lambda_j$  produce $p(z_i=j)$ or $x_i|z_i=j \\sim {\\cal N}(\\mu_j, \\Sigma_j)$.\n",
    "\n",
    "The parameters of our problem are $\\lambda$, $\\mu$ and $\\Sigma$. We can estimate them \n",
    "by minimizing the log-likelihood \n",
    "\\begin{eqnarray}\n",
    "l(x| \\lambda, \\mu, \\Sigma) &=& \\sum_{i=1}^{m} \\log p(x_i| \\lambda,  \\mu ,\\Sigma)   \\nonumber \\\\ \n",
    "     &=& \\sum_{i=1}^{m} \\log \\zsum p(x_i| z_i,  \\mu , \\Sigma) \\, p(z_i| \\lambda_i)  \n",
    "\\end{eqnarray} \n",
    "\n",
    "However, if we set to zero the derivatives of this formula with respect to\n",
    "the parameters and try to solve, we'll find that it is not possible to find the\n",
    "maximum likelihood estimates of the parameters in closed form. (Try it !!!)\n",
    "\n",
    "The variables $z_i$'s tells us which of the $k$ Gaussians $x_i$ are from. \n",
    "Of course if we knew $z$, then we have the supervised learning problem we solved above. We might think we could use regular optimization to find the MLE or MAP estimate, but we have a problem. We have to enforce constraints such as mixing weights summing to 1, covariance matrices being positive definite, etc. \n",
    "\n",
    "For all of these reasons, its simpler, but not always faster to use an iterative algorithm called the EM algorithm to get the local maximum likelihood or MAP estimate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The EM algorithm\n",
    "\n",
    "** Expectation-maximization (EM)** method is this iterative method for maximizing difficult \n",
    "likelihood (or posterior) problems. It was first introduced by Dempster, Laird, and Rubin (1977). \n",
    "\n",
    "EM recognizes that if the data were fully observed, then  ML/ MAP estimates would be easy to compute. It thus alternates between inferring the missing values given the parameters (E step), and then optimizing the parameters given the “filled in” data (M step). \n",
    "The idea is to find a lower-bound \n",
    "on the log-likelihood $\\ell$ (E-step) and the optimize the lower-bound (M-step). \n",
    "\n",
    "TODO: connection to data augmentation\n",
    "\n",
    "Suppose we have an estimation problem in which we have data consising of $m$ independent examples $\\{x_1,\\ldots,x_m\\}$ . \n",
    "The goal is to fit the parameters of the model, where the log-likelihood is given by \n",
    "\\begin{eqnarray}\n",
    "\\ell(x | \\theta)&=& \\log \\prod_{i=1}^{m} p(x_i| \\theta) =   \\sum_{i=1}^{m} \\log \\,p(x_i| \\theta)  \\\\ \n",
    "   &=& \\sum_{i=1}^{m} \\log \\zsumi \\,p(x_i,z| \\theta)  \\\\ \n",
    "\\end{eqnarray}\n",
    "\n",
    "where the $z$ are the latent random variables. If $z$ were observed then the maximum likelihood estimation would be easy. \n",
    "\n",
    "Indeed then, let us start with the full data log-likelihood, \n",
    "\n",
    "$$\\ell(x, z | \\theta) = \\sum_{i=1}^{m}  \\log \\,p(x_i, z_i | \\theta),$$\n",
    "\n",
    " which is the log-likelihood we'd calculate if we knew all the $z_i$. But we do not know thse, so lets assume the $\\{z_i\\}$ have some normalized distribution $q(z)$, and calculate the expected value of the full data log likelihood with respect to this distribution:\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathrm{E_q[\\ell( x,z | \\theta)]}  &=& \\sum_i \\zsumi q_{i}(z_i) \\log \\,p(x_i, z_i | \\theta)\\\\\n",
    "    &=& \\sum_i \\zsumi q_{i}(z_i) \\log \\,\\frac{p(x_i, z_i | \\theta)}{q_{i}(z_i)} +  \\sum_i \\zsumi q_{i}(z_i) \\log \\,q_{i}(z_i)\n",
    "\\end{eqnarray}\n",
    "\n",
    "The second term only involves $q$ and is independent of $\\theta$. Looking only at the first term inside the i-summation:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathcal{L}(i, q, \\theta) &=&  \\zsumi q_{i}(z_i) \\log \\,\\frac{p(x_i, z_i | \\theta)}{q_{i}(z_i)} \\\\\n",
    "&=& \\zsumi  q_i(z_i) \\left( \\log \\frac{p(z_i| x_i,  \\theta)}{ q_i(z_i)} + \\log p(x_i | \\theta)\\right)\n",
    "\\end{eqnarray}\n",
    "\n",
    "we can see that, since $\\zsumi q_i(z_i) = 1$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathcal{L}(i, q, \\theta) &=& \\zsumi  q_i(z_i) \\left( \\log \\frac{p(z_i| x_i,  \\theta)}{ q_i(z_i)} + \\log p(x_i | \\theta)\\right)\\\\\n",
    "    &=& -\\mathrm{KL}\\left(q_i || p_i \\right) + \\log p(x_i | \\theta)\\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $\\mathrm{KL}$ is the Kullback-Leibler divergence between $q(x)$ and the hidden variable posterior distribution $p(z|x,\\theta)$.\n",
    "\n",
    "Since the sum over the data-points of the second term is just the log-likelihood we desire, it can then can be written as:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\ell(x | \\theta) &=& \\sum_i \\left(\\mathcal{L}(i, q, \\theta) +\\mathrm{KL}\\left(q_i  || p_i \\right)\\right)\\\\\n",
    "&=& \\mathcal{L}(q, \\theta) + \\mathrm{KL}\\left(q || p \\right)\n",
    "\\end{eqnarray}\n",
    "\n",
    "where we are defining:\n",
    "\n",
    "$$\\mathrm{KL}(q || p) = \\sum_i \\mathrm{KL}\\left(q_i  || p_i \\right)$$\n",
    "\n",
    "as the sum of the KL-divergence at each data point, and $\\mathcal{L}(q, \\theta)$ as the sum of $\\mathcal{L}$ at each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(\"./klsplitup.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now recall from your statistics or Machine-learning class (or see the appendix\n",
    "below), that the Kullback Liebler divergence at each $x_i$ is 0 only if the distributions as a function of $z$ are the same; it is otherwise **ALWAYS** greater than 0. This tells us that the quantity $\\mathcal{L}(q, \\theta)$ (which is the expected value of the full-data log-likelihood minus the entropy of $q$), is **ALWAYS** smaller than or equal to the log-likelihood of $p(x | \\theta)$, as illustrated above. In other words, $\\mathcal{L}(q, \\theta)$ is a lower bound on the log-likelihood.\n",
    "\n",
    "This observation sets up the EM algorithm for us. If we choose, in the **E-step**, at some value of the parameters $\\theta_{old}$,\n",
    "\n",
    "$$q(z) = p(z | x, \\theta_{old}),$$ \n",
    "\n",
    "at each point $x_i$, we then set set the Kullback Liebler divergence to 0, and thus $\\mathcal{L}(q, \\theta)$ to the log-likelihood at $\\theta_{old}$,  and maximizing the lower bound. Indeed, one can think of this process as one of maximizing $\\mathcal{L}(q, \\theta)$ with respect to the function $q$. This is a functional maximization (physicists will have seen this in the calculus of variations which goes into Lagrangian formulation of mechanics and path integrals), and is the basis for an entire subfield of Bayesian inference called Variational Inference.\n",
    "\n",
    "The E stands for “expectation”, because it gives us the conditional probabilities of different values of Z, and probabilities are expectations of indicator functions. The E-step is illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(\"./klsplitestep.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the **M-step**. Since after the E-step, the lower bound touches the log-likelihood, any maximization of this lower bound from its current value with respect to $\\theta$ will also “push up” on the function itself. Thus M step guaranteedly modifies the parameters $\\theta$ to increase (or keep same) the likelihood of the observed data.\n",
    "\n",
    "Thus we hold now the distribution $q(z)$ fixed at the hidden variable posterior calculated at $\\theta_{old}$, and maximize $\\mathcal{L}(q, \\theta)$ with respect to $\\theta$ to obtain new parameter values $\\theta_{new}$. This is a regular maximization.\n",
    "\n",
    "The distribution $q$, calculated as it is at $\\theta_{old}$ will not equal the new posterior distribution $p(z|x,\\theta_{new})$, and hence there will be a nonzero KL divergence. Thus the increase in the log-likelihood will be greater than the increase in the lower bound $\\mathcal{L}$, as illustrated below.\n",
    "\n",
    "The M in “M-step” and “EM” stands for “maximization”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(\"./klsplitmstep.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since $\\mathcal{L}$ is maximized with respect to $\\theta$, one can equivalently maximize the expectation of the full-data log likelihood $\\mathrm{E_q[\\ell( x,z | \\theta)]}$ in the M-step since the difference is purely a function of $q$. Furthermore, if the joint distribution $p(x, z| \\theta)$ is a member of the exponential family, the log-likelihood will have a particularly simple form and will lead to a much simpler maximization than that of the incomple-data log-likelihood $p(x|\\theta)$.\n",
    "\n",
    "We now set $\\theta_{old} = \\theta_{new}$ and repeat the process. This **EM algorithm** is presented and  illustrated below:\n",
    "\n",
    "1. We start with the log-likelihood $p(x | \\theta)$(red curve) and the initial guess $\\theta_{old}$ of the parameter values\n",
    "2. Until convergence (the $\\theta$ values dont change too much):\n",
    "    1. E-step: Evaluate the hidden variable posterior $q(z, \\theta_{old}) = p(z | x, \\theta_{old})$ which gives rise to a lower bound function of $\\theta$: $\\mathcal{L}(q(z, \\theta_{old}), \\theta)$(blue curve) whose value equals the value of $p(x | \\theta) at $\\theta_{old}.\n",
    "    2. M-step: maximize the lower bound function with respect to $\\theta$ to get $\\theta_{new}$.\n",
    "    3. Set $\\theta_{old} = \\theta_{new}$\n",
    "    \n",
    "One iteration more is illustrated below, where the subsequent E-step constructs a new lower-bound function that is tangential to the log-likelihood at $\\theta_{new}$, and whose value at $\\theta_{new}$ is higher than the lower bound at $\\theta_{old}$ from the previous step.\n",
    "\n",
    "Thus\n",
    "\n",
    "$$\\ell(\\theta_{t+1}) \\ge \\mathcal{L}(q(z,\\theta_t), \\theta_{t+1}) \\ge \\mathcal{L}(q(z,\\theta_t), \\theta_{t}) = \\ell(\\theta_t)$$\n",
    "\n",
    "The first equality follows since $\\mathcal{L}$ is a lower bound on $\\ell$, the second from the M-step's mazimization of $\\mathcal{L}$, and the last from the vanishing of the KL-divergence after the E-step. As a consequence, you **must** observe monotonic increase of the observed-data log likelihood $\\ell$ across iterations. This is a  powerful debugging tool for your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(\"./emupdate.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that as shown above, since each EM iteration can only improve the likelihood, you are guaranteeing convergence to a local maximum. Because it **IS** local (why?), you must try some different initial values of $\\theta_{old}$ and take the one that gives you the largest $\\ell$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gaussian Mixture model using EM\n",
    "\n",
    "The EM algorithm comes to the rescue. As described above here is the algorithm:\n",
    "\n",
    "\n",
    "* Repeat until convergence \n",
    "*  E-step: For each $i,j$ calculate \n",
    "\n",
    "$$ w_{i,j} = q_i(z_i=j)=p(z_i=j| x_i, \\lambda, \\mu, \\Sigma) $$\n",
    "     \n",
    "* M-step: We need to maximize, with respect to our parameters the\n",
    "  \n",
    "\\begin{eqnarray}\n",
    " \\mathcal{L} &=& \\sum_i \\sum_{z_i} q_i(z_i) \\log \\frac{p(x_i,z_i |\\lambda, \\mu, \\Sigma)}{q_i(z_i)} \\nonumber \\\\\n",
    " \\mathcal{L} &=& \\sum_i \\sum_{j=i}^{k}  q_i(z_i=j) \\log \\frac{p(x_i|z_i=j , \\mu, \\Sigma) p(z_i=j|\\lambda)}{q_i(z_i=j)} \\\\\n",
    " \\mathcal{L} & =&  \\sum_{i=1}^{m} \\sum_{j=i}^{k} w_{i,j}  \\log \\left[   \\frac{ \\frac{1}{ (2\\pi)^{n/2}|\\Sigma_j|^{1/2}} \\exp \\left(    -\\frac{1}{2}(x_i-\\mu_j)^T \\Sigma_j^{-1} (x_i-\\mu_j) \\right)  \\, \\lambda_j   }{w_{i,j}}\\right]\n",
    "\\end{eqnarray}\n",
    "\n",
    "Taking the derivatives yields the following updating formulas:\n",
    "\n",
    "\\begin{eqnarray}\n",
    " \\lambda_j &=& \\frac{1}{m} \\sum_{i=1}^m w_{i,j} \\nonumber \\\\ \n",
    " \\mu_j&=& \\frac{ \\sum_{i=1}^m  w_{i,j} \\, x_i}{ \\sum_{i=1}^m  w_{i,j}} \\nonumber \\\\ \n",
    " \\Sigma_j &=& \\frac{ \\sum_{i=1}^m  w_{i,j} \\, (x_i-\\mu_j)(x_i-\\mu_j)^T}{ \\sum_{i=1}^m  w_{i,j}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "To calculate the E-step we basically calculating the posterior of the  $z$'s given the $x$'s and the\n",
    "current estimate of our parameters. We can use Bayes rule \n",
    "\n",
    "$$ w_{i,j}= p(z_i=j| x_i, \\lambda, \\mu, \\Sigma) = \\frac{p( x_i| z_i=j,  \\mu, \\Sigma)\\, p(z_i=j|\\lambda)}{\\sum_{l=1}^k p(x_i | z_i=l,  \\mu, \\Sigma) \\, p(z_i=l|\\lambda)} $$\n",
    "\n",
    "Where $p(x_i | z_i =j,  \\mu, \\Sigma)$ is the density of the Gaussian with mean $\\mu_j$ and covariance \n",
    "$\\Sigma_j$ at $x_i$ and $p(z_i=j| \\lambda)$ is simply $\\lambda_j$. \n",
    "If we to compare these formulas in the M-step with the ones we found in GDA we can see\n",
    "that are very similar except that instead of using $\\delta$ functions we use the $w$'s. Thus the EM algorithm corresponds here to a weighted maximum likelihood and the weights are interpreted as the `probability' of coming from that Gaussian instead of the deterministic \n",
    "$\\delta$ functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from Bios366 lecture notes\n",
    "from scipy.stats.distributions import norm\n",
    "\n",
    "def Estep(x, mu, sigma, lam):\n",
    "    a = lam * norm.pdf(x, mu[0], sigma[0])\n",
    "    b = (1. - lam) * norm.pdf(x, mu[1], sigma[1])\n",
    "    return b / (a + b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xrng = np.linspace(-5,5)\n",
    "plt.plot(xrng, Estep(xrng, mu_true, sigma_true, lambda_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Mstep(x, w):\n",
    "    lam = np.mean(1.-w) \n",
    "    \n",
    "    mu = [np.sum((1-w) * x)/np.sum(1-w), np.sum(w * x)/np.sum(w)]\n",
    "    \n",
    "    sigma = [np.sqrt(np.sum((1-w) * (x - mu[0])**2)/np.sum(1-w)), \n",
    "             np.sqrt(np.sum(w * (x - mu[1])**2)/np.sum(w))]\n",
    "    \n",
    "    return mu, sigma, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print lambda_true, mu_true, sigma_true\n",
    "# Initialize values\n",
    "mu = np.random.normal(4, 10, size=2)\n",
    "sigma = np.random.uniform(0, 5, size=2)\n",
    "lam = np.random.random()\n",
    "\n",
    "# Stopping criterion\n",
    "crit = 1e-15\n",
    "\n",
    "# Convergence flag\n",
    "converged = False\n",
    "\n",
    "# Loop until converged\n",
    "iterations=1\n",
    "\n",
    "\n",
    "while not converged:\n",
    "    # E-step\n",
    "    if np.isnan(mu[0]) or np.isnan(mu[1]) or np.isnan(sigma[0]) or np.isnan(sigma[1]):\n",
    "        print \"Singularity!\"\n",
    "        break\n",
    "        \n",
    "    w = Estep(x, mu, sigma, lam)\n",
    "\n",
    "    # M-step\n",
    "    mu_new, sigma_new, lam_new = Mstep(x, w)\n",
    "    \n",
    "    # Check convergence\n",
    "    converged = ((np.abs(lam_new - lam) < crit) \n",
    "                 & np.all(np.abs((np.array(mu_new) - np.array(mu)) < crit))\n",
    "                 & np.all(np.abs((np.array(sigma_new) - np.array(sigma)) < crit)))\n",
    "    mu, sigma, lam = mu_new, sigma_new, lam_new\n",
    "    iterations +=1           \n",
    "\n",
    "print \"Iterations\", iterations\n",
    "print('A: N({0:.4f}, {1:.4f})\\nB: N({2:.4f}, {3:.4f})\\nlam: {4:.4f}'.format(\n",
    "                        mu_new[0], sigma_new[0], mu_new[1], sigma_new[1], lam_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Faithful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ofdata=pd.read_csv(\"./oldfaithful.csv\")\n",
    "ofdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(ofdata.eruptions, ofdata.waiting);\n",
    "plt.xlabel('eruptions length')\n",
    "plt.ylabel('time between eruptions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "clf = mixture.GMM(n_components=2)\n",
    "clf.fit(ofdata.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask = clf.predict(ofdata.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx = np.linspace(0, 6) \n",
    "yy = np.linspace(40, 100) \n",
    "X, Y = np.meshgrid(xx, yy)\n",
    "XX = np.c_[X.ravel(), Y.ravel()]\n",
    "Z =  np.log(-clf.score_samples(XX)[0])\n",
    "Z = Z.reshape(X.shape)\n",
    "\n",
    "CS = plt.contour(X, Y, Z)\n",
    "CB = plt.colorbar(CS, shrink=0.8, extend='both')\n",
    "plt.scatter(ofdata.eruptions, ofdata.waiting, c=mask);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Why is EM important?\n",
    "\n",
    "We have motivated the EM algorithm using mixture models, but that is not its only place of use. \n",
    "\n",
    "Since MLE's can overfit, we often prefer to use MAP estimation. EM is a perfectly reasonable method for MAP estimation in mixture models; you just need to multiply in the prior.\n",
    "\n",
    "Basically the EM algorithm has a similar setup to the data augmentation problem and can be used in any problem which has a similar structure. Suppose for example you have two parameters $\\phi$ and $\\gamma$ in a posterior estimation, with daya $y$. Say that we'd like to estimate the posterior $p(\\phi | y)$. It may be relatively hard to estimate this, but suppose we can  work with $p(\\phi | \\gamma, y)$ and $p(\\gamma | \\phi, y)$. Then you can use the structure of the EM algorithm to estimate the marginal posterior of any one parameter. Start with:\n",
    "\n",
    "$$log p(\\phi | y) = log p(\\gamma, \\phi | y) - log p(\\gamma | \\phi, y)$$\n",
    "\n",
    "Notice the similarity of this to the above expressions with $\\phi$ as $x$, $y$ as $\\theta$, and $\\gamma$ as $z$. Thus the same derivations apply toany problem with this structure.\n",
    "\n",
    "This structure can also be used in type-2 likelihood or emprical bayes estimation (See Casella 2001) of hyperparameters in hierarchical models. The structure there is identical to the gibbs sampler for the hierarchical model (which has a structure identical to the data augmentation setup). See the hierarchical example in the Gibbs Sampler lecture for an example. How would you estimate $\\alpha$ and $\\beta$ there using empirical bayes, rather than the full Gibbs sampler for $\\{\\theta_k\\}$, $\\alpha$, and $\\beta$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Appendices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Appendix 1: Jensen's Inequality\n",
    "\n",
    "Let $f$ be a function with domain the set of real numbers. If the second derivative is greater than zero \n",
    "for all $x\\in R$ this function is convex\\footnote{In the case of vector inputs this is called positive \n",
    "semi-definite}. \n",
    "\n",
    "**Theorem**. Let $f$ be a convex function, and $X$ be a random variable, then\n",
    "\n",
    "$$ E[f(X)] \\ge f(E[X]) $$ \n",
    "\n",
    "Furthermore, if $f$ is stricly convex (i.e. $f''(x)>0$), then $E[f(x)]=f(E[X])$ only if $X=E[X]$ with\n",
    "probability 1 ($X$ is constant).  \n",
    "\n",
    "To see this lets first take the case of two random variables $x_1$ and $x_2$.\n",
    "\n",
    "\n",
    "**Defnition 1** Let f be a real valued function defined on an interval $I = [a, b]$.\n",
    "$f$ is said to be convex on I if $\\forall x_1, x_2 \\in I, \\lambda \\in [0, 1]$,\n",
    "\n",
    "\\begin{equation}\n",
    "f(\\lambda x_1 + (1 - \\lambda)\\,x_2) \\le \\lambda f(x_1) + (1- \\lambda)\\,f(x_2).\n",
    "\\end{equation}\n",
    "\n",
    "$f$ is said to be strictly convex if the inequality is strict. Intuitively, this definition\n",
    "states that the function falls below  the\n",
    "straight line (the secant) from points $(x_1, f(x_1))$ to $(x_2, f(x_2))$. In other words, the equality is satisfied only for $\\lambda = 0$ and $\\lambda = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"EM1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jensen's inequality** Let $f$ be a convex function defined on an interval $I$. \n",
    "If $x_1,x_2,\\dots,x_n \\in I {\\rm and} \\lambda_1, \\lambda_2,\\ldots,\\lambda_n \\ge  0$ with $\\sum^n_{i=1} \\lambda_i = 1$,\n",
    "\n",
    "\\begin{equation}\n",
    "f \\left( \\sum_{i=1}^n \\lambda_i \\,  x_i \\right) \\le  \\sum_{i=1}^n \\lambda_i f(x_i) \n",
    "\\end{equation}\n",
    "\n",
    "**Proof:**\tFor $n = 1$ this is trivial. The case $n = 2$ corresponds to the definition of convexity (see above). \n",
    "To show that this is true for all natural numbers, we proceed by induction. Assume the theorem is true for some $n$ then,\n",
    "\n",
    "\\begin{eqnarray}\n",
    "f \\left( \\sum_{i=1}^{n+1} \\lambda_i \\,  x_i \\right) &=& f\\left( \\lambda_{n+1} x_{n+1} + \\sum_{i=1}^n \\lambda_i \\,  x_i  \\right) \\nonumber \\\\\n",
    "\t\t&=&  f\\left( \\lambda_{n+1} x_{n+1} + \\frac{(1-\\lambda_{n+1})}{(1-\\lambda_{n+1})}\\sum_{i=1}^n \\lambda_i \\,  x_i  \\right) \\nonumber \\\\\n",
    "\t\t& \\le & \\lambda_{n+1} f(x_{n+1}) + (1-\\lambda_{n+1}) f \\left( \\frac{1}{(1-\\lambda_{n+1})} \\sum_{i=1}^n \\lambda_i \\,  x_i  \\right) \\nonumber \\\\\n",
    "\t\t& = & \\lambda_{n+1} f(x_{n+1}) + (1-\\lambda_{n+1}) f \\left( \\sum_{i=1}^n \\frac{\\lambda_i}{(1-\\lambda_{n+1})} \\,  x_i  \\right)  \\nonumber \\\\\n",
    "\t\t& \\le & \\lambda_{n+1} f(x_{n+1}) + (1-\\lambda_{n+1})  \\sum_{i=1}^n \\frac{\\lambda_i}{(1-\\lambda_{n+1})} \\,  f(x_i)  \\nonumber \\\\ \n",
    "\t\t& =&  \\lambda_{n+1} f(x_{n+1}) + \\sum_{i=1}^n \\lambda_i f(x_i) \\nonumber \\\\ \n",
    "\t\t& =&  \\sum_{i=1}^{n+1}  \\lambda_i f(x_i)\n",
    "\\end{eqnarray}\n",
    "\n",
    "By interpreting the $\\lambda_i$ as the probability distribution over a discrete variable $x$ taking the values $\\{x_i\\}$:\n",
    "\n",
    "$$f(\\mathrm{E}[x]) \\le \\mathrm{E}[f(x)]$$\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix 2: The Kullback-Liebler divergence. (taken from Bishop)\n",
    "\n",
    "Consider some unknown distribution $q(x)$, and suppose that we have modelled this using an approximating distribution $p(x)$. The average additional amount of information required to specify the value of $x$  as a result of using $q(x)$ instead of the true distribution $p(x)$ is given by\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathrm{KL}(q || p) &=& -\\sum_{x} q(x) \\log p(x)  - \\left(-\\sum_x p(x) \\log p(x) \\right)\\\\\n",
    "    &=& \\sum_x q(x) \\log \\frac{q(x)}{p(x)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "￼This is known as the relative entropy or Kullback-Leibler divergence, or $\\mathrm{KL}$-divergence (Kullback and Leibler, 1951), between the distributions $p(x$) and $q(x)$. Note that it is not a symmetrical quantity.\n",
    "\n",
    "We can use Jensen's inequality on a convex function $f(x)$:\n",
    "\n",
    " to show that $\\mathrm{KL}(q || p) \\ge 0$ with equality iff $p=q$.\n",
    "\n",
    "$$\\mathrm{KL}(q || p) = - \\sum_x q(x) \\log \\frac{p(x)}{q(x)} \\ge \\log \\left( \\sum_x p(x) \\right) = 0$$\n",
    "\n",
    "where we have used the fact that $-log(x)$ is a convex function, and that $p(x)$ normalizes to a distribution. Infact, since $=\\log(x)$ is strictly convex, the equality only happens if $p(x) = q(x)$ for ALL x.\n",
    "\n",
    "Thus we can interpret the Kullback-Leibler divergence as a measure of the dissimilarity of the two distributions p(x) and q(x). In Bayesian statistics the KL divergence can be used as a measure of the information gain in moving from a prior to posterior, and common goal in Bayesian experimental design is to maximise the expected KL divergence between the prior and the posterior. The divergence is also used to understand mutual information in clustering. It is also used in variational bayesian inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
